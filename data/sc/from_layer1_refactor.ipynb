{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c46d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mlaas_tools2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8866750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T10:20:49.479046Z",
     "start_time": "2022-02-16T10:20:48.207819Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlaas_tools2.db_tool import DatabaseConnections\n",
    "from mlaas_tools2.config_info import ConfigPass\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DateType, StructType, StructField, StringType, LongType, DoubleType\n",
    "from pyspark.sql.functions import create_map, lit\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime,  timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b9363",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Spark 連線"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d6bdd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T10:20:53.413498Z",
     "start_time": "2022-02-16T10:20:49.605534Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-esb22046:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>T</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f909f737e50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default = {\"spark.driver.memory\":'16g',\n",
    "           \"fs.s3a.access.key\": \"smartchannel\",\n",
    "           \"fs.s3a.secret.key\": \"smartchannel\",\n",
    "           \"fs.s3a.endpoint\": \"http://10.240.205.23:9000\",\n",
    "           \"fs.s3a.connection.ssl.enabled\": False,\n",
    "           \"fs.s3a.path.style.access\": True,} \n",
    "spark = SparkSession.builder.config(\n",
    "    conf = (SparkConf().setAppName(\"T\").setAll(default.items()))).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084963ea",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 從feature group 以及 layer1 讀資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "414f4093",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_date = '20211224'\n",
    "train_end_date = '20211225'\n",
    "test_date = '20211226'\n",
    "# test_date = '20220131'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6e6a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_date = '20211217'\n",
    "train_end_date = '20211226'\n",
    "test_date = '20211227'\n",
    "# test_date = '20220131'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7107c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_date = '20220227'\n",
    "train_end_date = '20220227'\n",
    "test_date = '20220228'\n",
    "# test_date = '20220131'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e973f7a1",
   "metadata": {},
   "source": [
    "## 1. item-subtag (sc_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c2c5f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sdf = (\n",
    "    spark.read.parquet(\n",
    "        \"s3a://df-smart-channel/recsys-dataset/beta_v2/layer1/sc_item\"\n",
    "    ).where(col('service')=='smart_channel').\n",
    "    where(\n",
    "        (col(\"date\")>=train_start_date) & (col(\"date\")<=train_end_date)\n",
    "    )\n",
    "    # .select(['item_id', 'date','subtag_list'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc0b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_subtag_exploded = item_sdf.select(item_sdf.item_id, item_sdf.date,  F.explode(item_sdf.subtag_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e74212ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_subtag_details = item_subtag_exploded.select(['item_id', 'date', 'col.subtag' ,'col.subtag_chinese_desc','col.subtag_eng_desc']).filter(item_subtag_exploded['col.subtag_chinese_desc'].startswith('03_'))\n",
    "item_subtag_sdf = item_subtag_details.select(['item_id', 'date', 'subtag_eng_desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6590328",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_subtag_sdf_train = item_subtag_sdf.filter(item_subtag_sdf.date == train_end_date)\n",
    "# item_subtag_net_test = item_subtag_net.filter(item_subtag_net.date == test_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_subtag_sdf_train.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a68c3b",
   "metadata": {},
   "source": [
    "### 2.user-item (context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d60ff30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sdf = (\n",
    "    spark.read.parquet(\n",
    "        \"s3a://df-smart-channel/recsys-dataset/beta_v2/layer1/context\"\n",
    "    ).where(\n",
    "        (col(\"date\")>=train_start_date) & (col(\"date\")<=train_end_date)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d56f2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除沒有點擊紀錄後context的數目: 294509\n",
      "展開item後context的數目: 629533\n",
      "選 df_smart_channel後context的數目: 485314\n",
      "過濾掉公版後context的數目: 485314\n",
      "Total Context number: 485314\n"
     ]
    }
   ],
   "source": [
    "# 去除沒有點擊紀錄的\n",
    "clean_context_sdf = context_sdf.where(col('item_click_list').isNotNull())\n",
    "print('去除沒有點擊紀錄後context的數目:', clean_context_sdf.count())\n",
    "# 將item_click_list展開\n",
    "clean_context_sdf = clean_context_sdf.select(clean_context_sdf.cust_no, \n",
    "                                             clean_context_sdf.date,\n",
    "                                             F.explode(clean_context_sdf.item_click_list))\n",
    "# 選出欄位\n",
    "flatten_clean_context_sdf = clean_context_sdf.select(F.col('cust_no'), F.col('date'), F.col('col.*'))\n",
    "print('展開item後context的數目:', flatten_clean_context_sdf.count())\n",
    "# 選 df_smart_channel\n",
    "flatten_clean_context_sdf = flatten_clean_context_sdf.where(col('hits_eventinfo_eventcategory')=='smart_channel')\n",
    "print('選 df_smart_channel後context的數目:', flatten_clean_context_sdf.count())\n",
    "#和item sdf合併\n",
    "flatten_clean_context_sdf = flatten_clean_context_sdf.join(item_sdf.drop('click', 'show'), on=['item_id', 'date'], how='left')\n",
    "# 過濾掉公版\n",
    "# flatten_clean_context_sdf = flatten_clean_context_sdf.where(col('service')!='public_content')\n",
    "print('過濾掉公版後context的數目:', flatten_clean_context_sdf.count())\n",
    "flatten_clean_context_sdf = flatten_clean_context_sdf.select(F.col('item_id'), F.col('cust_no'), F.col('date'),\n",
    "                               F.col('visitdatetime'), F.col('eventdatetime'), F.col('click'), F.col('show')\n",
    "                              )\n",
    "# 找出最先點擊或最先曝光(一天可能點同一個item多次)\n",
    "flatten_clean_context_sdf = flatten_clean_context_sdf.withColumn(\"day_order\", F.row_number().over(Window.partitionBy(['cust_no', 'date', 'item_id']).orderBy(flatten_clean_context_sdf['eventdatetime'])))\n",
    "# 計算該顧客在當天點同一個item的點擊和曝光總數\n",
    "cust_click_bydate = flatten_clean_context_sdf.groupby(['cust_no', 'date', 'item_id']).sum('click').withColumnRenamed('sum(click)', 'click')\n",
    "cust_show_bydate = flatten_clean_context_sdf.groupby(['cust_no', 'date', 'item_id']).sum('show').withColumnRenamed('sum(show)', 'show')\n",
    "# 選最先點擊的item\n",
    "flatten_clean_context_sdf = flatten_clean_context_sdf.where(col('day_order')==1).drop('click', 'show', 'day_order')\n",
    "flatten_clean_context_sdf = flatten_clean_context_sdf.join(cust_click_bydate, on=['cust_no', 'date', 'item_id'], how='left')\n",
    "flatten_clean_context_sdf = flatten_clean_context_sdf.join(cust_show_bydate, on=['cust_no', 'date', 'item_id'], how='left')\n",
    "# 計算點擊排名。挑出有點擊的人做排名 15674人有點擊 rank最多為2\n",
    "click_sdf = flatten_clean_context_sdf.where(col('click')>0)\n",
    "click_sdf = click_sdf.withColumn(\"rank\", F.row_number().over(Window.partitionBy(['cust_no', 'date']).orderBy(click_sdf['eventdatetime'])))\n",
    "flatten_clean_context_sdf = flatten_clean_context_sdf.join(click_sdf.select('click', 'cust_no', 'date', 'item_id', 'rank'), on=['click', 'cust_no', 'date', 'item_id'], how='left')\n",
    "print('Total Context number:', flatten_clean_context_sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f96b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sdf_train = flatten_clean_context_sdf.filter(col('date') <= train_end_date)\n",
    "# context_sdf_test = flatten_clean_context_sdf.filter(col('date') == test_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4f203ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueUsersObserved = context_sdf_train.select('cust_no').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6055b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users_list = [each_user.__getitem__('cust_no') for each_user in uniqueUsersObserved]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "019b78e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter unobserved in test\n",
    "# context_sdf_test = context_sdf_test.filter(context_sdf_test.cust_no.isin(unique_users_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446e64a",
   "metadata": {},
   "source": [
    "## 3. user-subtag (user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02a668e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T10:21:09.325986Z",
     "start_time": "2022-02-16T10:20:53.418343Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Sdf數目: 249725\n"
     ]
    }
   ],
   "source": [
    "user_sdf = (\n",
    "    spark.read.parquet(\n",
    "        \"s3a://df-smart-channel/recsys-dataset/beta_v2/layer1/user\"\n",
    "    ).where(\n",
    "        col(\"date\") == train_end_date\n",
    "        #(col(\"date\")>=train_end_date) & (col(\"date\")<=train_end_date\n",
    "    )\n",
    ")\n",
    "user_sdf = user_sdf.drop('click', 'show')\n",
    "user_sdf = user_sdf.select([\n",
    "    'cust_no', 'date', 'mobile_login_90', 'dd_my', 'dd_md', 'onlymd_ind', 'onlycc_ind', 'efingo_card_ind',  'cl_cpa_amt', 'fc_ind'\n",
    "]).filter(user_sdf.cust_no.isin(unique_users_list))\n",
    "print('User Sdf數目:', user_sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c493f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_subtag = user_sdf.filter(user_sdf.cust_no.isin(unique_users_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f8f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_df_train = user_sdf.filter(user_sdf.date == train_end_date).toPandas()\n",
    "# user_df_test = user_sdf_sub.filter(user_sdf_sub.date == test_date).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_file(user_df_test, test_date, 'user_subtag.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c19850",
   "metadata": {},
   "source": [
    "Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81101e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: string, date: int, subtag_eng_desc: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_subtag_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e28c83b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/date=20211226/item_subtag.parquet'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"/date={test_date}/item_subtag.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87430202",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_subtag_sdf.write.parquet(f\"/date={test_date}/item_subtag.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ba6b889",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o499753.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:848)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 1 times, most recent failure: Lost task 0.0 in stage 44.0 (TID 2171, jupyter-esb22046, executor driver): java.io.IOException: Mkdirs failed to create file:/date=20211226/context.parquet/_temporary/0/_temporary/attempt_20220401171424_0044_m_000000_2171 (exists=false, cwd=file:/project/df-smart-channel/graph/LightGCN-PyTorch/data/sc)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n\t... 33 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/date=20211226/context.parquet/_temporary/0/_temporary/attempt_20220401171424_0044_m_000000_2171 (exists=false, cwd=file:/project/df-smart-channel/graph/LightGCN-PyTorch/data/sc)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f2416f76765b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontext_sdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/date={test_date}/context.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o499753.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:848)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 1 times, most recent failure: Lost task 0.0 in stage 44.0 (TID 2171, jupyter-esb22046, executor driver): java.io.IOException: Mkdirs failed to create file:/date=20211226/context.parquet/_temporary/0/_temporary/attempt_20220401171424_0044_m_000000_2171 (exists=false, cwd=file:/project/df-smart-channel/graph/LightGCN-PyTorch/data/sc)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n\t... 33 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/date=20211226/context.parquet/_temporary/0/_temporary/attempt_20220401171424_0044_m_000000_2171 (exists=false, cwd=file:/project/df-smart-channel/graph/LightGCN-PyTorch/data/sc)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "context_sdf_train.write.parquet(f\"/date={test_date}/context.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13341d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sdf.write.parquet(f\"/date={test_date}/user.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sdf.write.parquet(\"/tmp/date/user.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a926c9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'date=20211226',\n",
       " 'date=20211227',\n",
       " 'date=20211228',\n",
       " 'date=20211229',\n",
       " 'date=20211230',\n",
       " 'date=20211231',\n",
       " 'smart_preprocess_dynamic.ipynb',\n",
       " 'smart_preprocess_static.ipynb',\n",
       " 'eda.ipynb',\n",
       " 'testcase',\n",
       " 'fake',\n",
       " 'from_layer1.ipynb',\n",
       " 'from_layer1.py',\n",
       " 'item_subtag.csv',\n",
       " 'preprocess_integrated.ipynb',\n",
       " 'preprocess_integrated.py',\n",
       " 'preprocess_subtag.ipynb',\n",
       " 'smart_preprocess_retain.ipynb',\n",
       " 'preprocess_refactor.ipynb',\n",
       " 'subtag_map.xlsx',\n",
       " 'from_layer1_refactor.ipynb',\n",
       " 'date=20220227',\n",
       " 'date=20220228',\n",
       " 'date=20220301',\n",
       " 'date=20220302']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d22a99f",
   "metadata": {},
   "source": [
    "## phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_subtag_sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07523517",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sdf_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9be17c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cust_no',\n",
       " 'date',\n",
       " 'mobile_login_90',\n",
       " 'dd_my',\n",
       " 'dd_md',\n",
       " 'onlymd_ind',\n",
       " 'onlycc_ind',\n",
       " 'efingo_card_ind',\n",
       " 'cl_cpa_amt',\n",
       " 'fc_ind']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_sdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f17ce",
   "metadata": {},
   "source": [
    "### get user-item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a207483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_user = LabelEncoder()\n",
    "context_train.cust_no = le_user.fit_transform(context_train.cust_no)\n",
    "# context_test.cust_no = le_user.transform(context_test.cust_no)\n",
    "user_subtag['cust_no'] = le_user.transform(user_subtag['cust_no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad6aa4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_indexer = StringIndexer(inputCol=\"cust_no\", outputCol=\"cust_id\") \n",
    "cust_transformer = cust_indexer.fit(context_sdf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ffe0ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sdf_train_indexed = cust_transformer.transform(context_sdf_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf615f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_indexer = StringIndexer(inputCol=\"item_id\", outputCol=\"item_ind\") \n",
    "item_transformer = item_indexer.fit(context_sdf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a54e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sdf_train_indexed = item_transformer.transform(context_sdf_train) \n",
    "item_subtag_sdf_train_indexed = item_transformer.transform(item_subtag_sdf_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelPath = temp_path + \"/string-indexer-model\"\n",
    "# model.save(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b475b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadedTransformer = StringIndexerModel.load(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be25649",
   "metadata": {},
   "source": [
    "### get user-subtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0c15f58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249725"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_subtag.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行銀活躍用戶\n",
    "mobile_active = user_subtag.mobile_login_90\n",
    "# 有外幣帳戶\n",
    "forex_digital_account = user_subtag.dd_my\n",
    "# 存戶\n",
    "account = user_subtag.dd_md\n",
    "# 純存戶\n",
    "account_only = user_subtag.onlymd_ind\n",
    "# 純卡戶\n",
    "credit_card_only = user_subtag.onlycc_ind\n",
    "# 卡存戶有e.Fingo指定卡\n",
    "card_pi_only_ubear = user_subtag.efingo_card_ind\n",
    "# 有信貸者\n",
    "personal_loan_account_cust = user_subtag.cl_cpa_amt.notna()\n",
    "# 無理專顧客\n",
    "no_fc_cust = user_subtag.fc_ind == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_subtag = user_subtag.sort_values('cust_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b109a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_subtag_dict = {}\n",
    "for i, ids in enumerate(user_subtag.cust_no):\n",
    "    subtags_list = []\n",
    "    if mobile_active[i]: subtags_list.append(0);\n",
    "    if forex_digital_account[i]: subtags_list.append(1);\n",
    "    if account[i]: subtags_list.append(2);\n",
    "    if account_only[i]: subtags_list.append(3);        \n",
    "    if credit_card_only[i]: subtags_list.append(4);\n",
    "    if card_pi_only_ubear[i]: subtags_list.append(5);\n",
    "    if personal_loan_account_cust[i]: subtags_list.append(6);\n",
    "    if no_fc_cust[i]: subtags_list.append(7);\n",
    "    user_subtag_dict[ids] = subtags_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd03d23",
   "metadata": {},
   "source": [
    "### get item-subtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18da85c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df = pd.read_excel('subtag_map.xlsx', sheet_name = 'mapping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a872c2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------------+--------+\n",
      "|             item_id|    date|   subtag_eng_desc|item_ind|\n",
      "+--------------------+--------+------------------+--------+\n",
      "|8236E47D-EC12-4B4...|20211225|03_wealth_view_ind|     9.0|\n",
      "|8236E47D-EC12-4B4...|20211225| 03_investment_ind|     9.0|\n",
      "|8236E47D-EC12-4B4...|20211225|   03_account_only|     9.0|\n",
      "|0266753B-F0CD-460...|20211225|03_wealth_view_ind|    17.0|\n",
      "|0266753B-F0CD-460...|20211225|     03_no_fc_cust|    17.0|\n",
      "+--------------------+--------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_subtag_sdf_train_indexed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3626c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstrip(column, n):\n",
    "    # should probably add error checking on inputs\n",
    "    return expr(\"substring(`{col}`, {n}+1, length(`{col}`)-{n})\".format(col=column, n=n))\n",
    "\n",
    "item_subtag_sdf_train_indexed = item_subtag_sdf_train_indexed.withColumn(\"subtag_eng_desc\", lstrip(column=\"subtag_eng_desc\", n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae18288c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------------+--------+-------------------+\n",
      "|             item_id|    date|subtag_eng_desc|item_ind|new_subtag_eng_desc|\n",
      "+--------------------+--------+---------------+--------+-------------------+\n",
      "|8236E47D-EC12-4B4...|20211225|wealth_view_ind|     9.0|    wealth_view_ind|\n",
      "|8236E47D-EC12-4B4...|20211225| investment_ind|     9.0|     investment_ind|\n",
      "+--------------------+--------+---------------+--------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_subtag_sdf_train_indexed.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_items = mapping_df.subtag_03.unique()\n",
    "item_subtag_sdf_train_indexed = item_subtag_sdf_train_indexed.withColumn(\"subtag_eng_desc\", lstrip(column=\"subtag_eng_desc\", n=3))\n",
    "item_subtag_sdf_train_indexed = item_subtag_sdf_train_indexed.filter(item_subtag_sdf_train_indexed.subtag_eng_desc.isin(list(available_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "60629c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------+--------------------+\n",
      "|             item_id|    date|     subtag_eng_desc|item_ind| new_subtag_eng_desc|\n",
      "+--------------------+--------+--------------------+--------+--------------------+\n",
      "|8236E47D-EC12-4B4...|20211225|        account_only|     9.0|        account_only|\n",
      "|0266753B-F0CD-460...|20211225|          no_fc_cust|    17.0|          no_fc_cust|\n",
      "|0266753B-F0CD-460...|20211225|             account|    17.0|             account|\n",
      "|B105ECE3-B0EA-4F0...|20211225|       mobile_active|    18.0|       mobile_active|\n",
      "|B105ECE3-B0EA-4F0...|20211225|forex_digital_acc...|    18.0|forex_digital_acc...|\n",
      "+--------------------+--------+--------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_subtag_sdf_train_indexed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "35a997f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: string, date: int, subtag_eng_desc: string, item_ind: double, new_subtag_eng_desc: string]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_sdf=spark.createDataFrame(mapping_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7f28a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_subtags = item_subtag_sdf_train_indexed.join(mapping_sdf, item_subtag_sdf_train_indexed.subtag_eng_desc == mapping_sdf.subtag_03, 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "72a9710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_subtags_df = item_subtags.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f10af67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_subtags = pd.merge(item_subtag, mapping_df, how = 'left', left_on = 'subtag_eng_desc', right_on = 'subtag_03', copy = False)\n",
    "subtags_by_item = item_subtags_df.groupby('item_ind')['code'].unique()\n",
    "item_subtag_dict = dict(subtags_by_item.apply(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3e3c4e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.0: [2, 7],\n",
       " 2.0: [4],\n",
       " 3.0: [1],\n",
       " 4.0: [1],\n",
       " 5.0: [6, 0, 1],\n",
       " 6.0: [5, 4],\n",
       " 7.0: [2, 7],\n",
       " 8.0: [2, 7],\n",
       " 9.0: [3],\n",
       " 12.0: [2, 4],\n",
       " 13.0: [2],\n",
       " 14.0: [2, 3],\n",
       " 15.0: [5, 3],\n",
       " 16.0: [4, 3],\n",
       " 17.0: [2, 7],\n",
       " 18.0: [6, 0, 1],\n",
       " 19.0: [2, 4, 3],\n",
       " 20.0: [5, 4]}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_subtag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484259f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_file(context_df_train, test_date, 'context_train.csv')\n",
    "# export_file(context_df_test, test_date, 'context_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8603c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b1f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b689bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
